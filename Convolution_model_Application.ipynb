{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Application\n",
    "\n",
    "Welcome to Course 4's second assignment! In this notebook, you will:\n",
    "\n",
    "- Create a mood classifer using the TF Keras Sequential API\n",
    "- Build a ConvNet to identify sign language digits using the TF Keras Functional API\n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "\n",
    "- Build and train a ConvNet in TensorFlow for a __binary__ classification problem\n",
    "- Build and train a ConvNet in TensorFlow for a __multiclass__ classification problem\n",
    "- Explain different use cases for the Sequential and Functional APIs\n",
    "\n",
    "To complete this assignment, you should already be familiar with TensorFlow. If you are not, please refer back to the **TensorFlow Tutorial** of the third week of Course 2 (\"**Improving deep neural networks**\").\n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "If you do any of the following, you will get something like, `Grader Error: Grader feedback not found` (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these [instructions](https://www.coursera.org/learn/convolutional-neural-networks/supplement/DS4yP/h-ow-to-refresh-your-workspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Packages](#1)\n",
    "    - [1.1 - Load the Data and Split the Data into Train/Test Sets](#1-1)\n",
    "- [2 - Layers in TF Keras](#2)\n",
    "- [3 - The Sequential API](#3)\n",
    "    - [3.1 - Create the Sequential Model](#3-1)\n",
    "        - [Exercise 1 - happyModel](#ex-1)\n",
    "    - [3.2 - Train and Evaluate the Model](#3-2)\n",
    "- [4 - The Functional API](#4)\n",
    "    - [4.1 - Load the SIGNS Dataset](#4-1)\n",
    "    - [4.2 - Split the Data into Train/Test Sets](#4-2)\n",
    "    - [4.3 - Forward Propagation](#4-3)\n",
    "        - [Exercise 2 - convolutional_model](#ex-2)\n",
    "    - [4.4 - Train the Model](#4-4)\n",
    "- [5 - History Object](#5)\n",
    "- [6 - Bibliography](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages\n",
    "\n",
    "As usual, begin by loading in the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "### v1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/vishan/.local/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: h5py in /home/vishan/.local/lib/python3.10/site-packages (3.11.0)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "Requirement already satisfied: Pillow in /home/vishan/.local/lib/python3.10/site-packages (10.3.0)\n",
      "Requirement already satisfied: pandas in /home/vishan/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vishan/.local/lib/python3.10/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/vishan/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vishan/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Collecting keras>=3.0.0\n",
      "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes~=0.3.1\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/vishan/.local/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.17,>=2.16\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/vishan/.local/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Collecting typeguard<3.0.0,>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting optree\n",
      "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting namex\n",
      "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /home/vishan/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vishan/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vishan/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vishan/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.1.1\n",
      "  Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vishan/.local/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, typeguard, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, scipy, protobuf, optree, opt-einsum, ml-dtypes, mdurl, MarkupSafe, markdown, kiwisolver, grpcio, google-pasta, gast, fonttools, cycler, contourpy, astunparse, absl-py, werkzeug, tensorflow-addons, matplotlib, markdown-it-py, tensorboard, rich, keras, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 contourpy-1.2.1 cycler-0.12.1 flatbuffers-24.3.25 fonttools-4.51.0 gast-0.5.4 google-pasta-0.2.0 grpcio-1.63.0 keras-3.3.3 kiwisolver-1.4.5 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 matplotlib-3.8.4 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 rich-13.7.1 scipy-1.13.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-addons-0.23.0 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0 typeguard-2.13.3 werkzeug-3.0.2 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy h5py matplotlib scipy Pillow pandas tensorflow tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 12:48:13.021399: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-04 12:48:13.021628: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-04 12:48:13.023622: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-04 12:48:13.048957: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-04 12:48:13.491082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cnn_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfl\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcnn_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtest_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary, comparator\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cnn_utils'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.python.framework import ops\n",
    "from cnn_utils import *\n",
    "from test_utils import summary, comparator\n",
    "import tensorflow.keras.models\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - Load the Data and Split the Data into Train/Test Sets\n",
    "\n",
    "You'll be using the Happy House dataset for this part of the assignment, which contains images of peoples' faces. Your task will be to build a ConvNet that determines whether the people in the images are smiling or not -- because they only get to enter the house if they're smiling!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_happy_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes \u001b[38;5;241m=\u001b[39m \u001b[43mload_happy_dataset\u001b[49m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Normalize image vectors\u001b[39;00m\n\u001b[1;32m      4\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train_orig\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_happy_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_happy_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# Reshape\n",
    "Y_train = Y_train_orig.T\n",
    "Y_test = Y_test_orig.T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display the images contained in the dataset. Images are **64x64** pixels in RGB format (3 channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m124\u001b[39m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mX_train_orig\u001b[49m[index]) \u001b[38;5;66;03m#display sample training image\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_orig' is not defined"
     ]
    }
   ],
   "source": [
    "index = 124\n",
    "plt.imshow(X_train_orig[index]) #display sample training image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Layers in TF Keras \n",
    "\n",
    "In the previous assignment, you created layers manually in numpy. In TF Keras, you don't have to write code directly to create layers. Rather, TF Keras has pre-defined layers you can use. \n",
    "\n",
    "When you create a layer in TF Keras, you are creating a function that takes some input and transforms it into an output you can reuse later. Nice and easy! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - The Sequential API\n",
    "\n",
    "In the previous assignment, you built helper functions using `numpy` to understand the mechanics behind convolutional neural networks. Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call. Keras is a high-level abstraction built on top of TensorFlow, which allows for even more simplified and optimized model creation and training. \n",
    "\n",
    "For the first part of this assignment, you'll create a model using TF Keras' Sequential API, which allows you to build layer by layer, and is ideal for building models where each layer has **exactly one** input tensor and **one** output tensor. \n",
    "\n",
    "As you'll see, using the Sequential API is simple and straightforward, but is only appropriate for simpler, more straightforward tasks. Later in this notebook you'll spend some time building with a more flexible, powerful alternative: the Functional API. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - Create the Sequential Model\n",
    "\n",
    "As mentioned earlier, the TensorFlow Keras Sequential API can be used to build simple models with layer operations that proceed in a sequential order. \n",
    "\n",
    "You can also add layers incrementally to a Sequential model with the `.add()` method, or remove them using the `.pop()` method, much like you would in a regular Python list.\n",
    "\n",
    "Actually, you can think of a Sequential model as behaving like a list of layers. Like Python lists, Sequential layers are ordered, and the order in which they are specified matters.  If your model is non-linear or contains layers with multiple inputs or outputs, a Sequential model wouldn't be the right choice!\n",
    "\n",
    "For any layer construction in Keras, you'll need to specify the input shape in advance. This is because in Keras, the shape of the weights is based on the shape of the inputs. The weights are only created when the model first sees some input data. Sequential models can be created by passing a list of layers to the Sequential constructor, like you will do in the next assignment.\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - happyModel\n",
    "\n",
    "Implement the `happyModel` function below to build the following model: `ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE`. Take help from [tf.keras.layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers) \n",
    "\n",
    "Also, plug in the following parameters for all the steps:\n",
    "\n",
    " - [ZeroPadding2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ZeroPadding2D): padding 3, input shape 64 x 64 x 3\n",
    " - [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D): Use 32 7x7 filters, stride 1\n",
    " - [BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization): for axis 3\n",
    " - [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)\n",
    " - [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D): Using default parameters\n",
    " - [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) the previous output.\n",
    " - Fully-connected ([Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer: Apply a fully connected layer with 1 neuron and a sigmoid activation. \n",
    " \n",
    " \n",
    " **Hint:**\n",
    " \n",
    " Use **tfl** as shorthand for **tensorflow.keras.layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95d28b191f257bdd5b70c7b8952559d5",
     "grade": false,
     "grade_id": "cell-0e56d3fc28b69aec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: happyModel\n",
    "\n",
    "def happyModel():\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE\n",
    "    \n",
    "    Note that for simplicity and grading purposes, you'll hard-code all the values\n",
    "    such as the stride and kernel (filter) sizes. \n",
    "    Normally, functions should take these values as function parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    model -- TF Keras model (object containing the information for the entire training process) \n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "            ## ZeroPadding2D with padding 3, input shape of 64 x 64 x 3\n",
    "            \n",
    "            ## Conv2D with 32 7x7 filters and stride of 1\n",
    "            \n",
    "            ## BatchNormalization for axis 3\n",
    "            \n",
    "            ## ReLU\n",
    "            \n",
    "            ## Max Pooling 2D with default parameters\n",
    "            \n",
    "            ## Flatten layer\n",
    "            \n",
    "            ## Dense layer with 1 unit for output & 'sigmoid' activation\n",
    "            \n",
    "            # YOUR CODE STARTS HERE\n",
    "    tf.keras.layers.ZeroPadding2D(padding=(3, 3), input_shape=(64, 64, 3)),\n",
    "    tf.keras.layers.Conv2D(32, (7, 7), strides=(1, 1)),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "            \n",
    "            # YOUR CODE ENDS HERE\n",
    "      \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d3575c950e2e78149be2d05d671c80d",
     "grade": true,
     "grade_id": "cell-e3e1046e5c33d775",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))]\n",
      "['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform']\n",
      "['BatchNormalization', (None, 64, 64, 32), 128]\n",
      "['ReLU', (None, 64, 64, 32), 0]\n",
      "['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid']\n",
      "['Flatten', (None, 32768), 0]\n",
      "['Dense', (None, 1), 32769, 'sigmoid']\n",
      "\u001b[32mAll tests passed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "happy_model = happyModel()\n",
    "# Print a summary for each layer\n",
    "for layer in summary(happy_model):\n",
    "    print(layer)\n",
    "    \n",
    "output = [['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))],\n",
    "            ['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform'],\n",
    "            ['BatchNormalization', (None, 64, 64, 32), 128],\n",
    "            ['ReLU', (None, 64, 64, 32), 0],\n",
    "            ['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid'],\n",
    "            ['Flatten', (None, 32768), 0],\n",
    "            ['Dense', (None, 1), 32769, 'sigmoid']]\n",
    "    \n",
    "comparator(summary(happy_model), output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "\n",
    "```\n",
    "['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))]\n",
    "['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform']\n",
    "['BatchNormalization', (None, 64, 64, 32), 128]\n",
    "['ReLU', (None, 64, 64, 32), 0]\n",
    "['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid']\n",
    "['Flatten', (None, 32768), 0]\n",
    "['Dense', (None, 1), 32769, 'sigmoid']\n",
    "All tests passed!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your model is created, you can compile it for training with an optimizer and loss of your choice. When the string `accuracy` is specified as a metric, the type of accuracy used will be automatically converted based on the loss function used. This is one of the many optimizations built into TensorFlow that make your life easier! If you'd like to read more on how the compiler operates, check the docs [here](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_model.compile(optimizer='adam',\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to check your model's parameters with the `.summary()` method. This will display the types of layers you have, the shape of the outputs, and how many parameters are in each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d (ZeroPadding2 (None, 70, 70, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        4736      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 32769     \n",
      "=================================================================\n",
      "Total params: 37,633\n",
      "Trainable params: 37,569\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "happy_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Train and Evaluate the Model\n",
    "\n",
    "After creating the model, compiling it with your choice of optimizer and loss function, and doing a sanity check on its contents, you are now ready to build! \n",
    "\n",
    "Simply call `.fit()` to train. That's it! No need for mini-batching, saving, or complex backpropagation computations. That's all been done for you, as you're using a TensorFlow dataset with the batches specified already. You do have the option to specify epoch number or minibatch size if you like (for example, in the case of an un-batched dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 4s 100ms/step - loss: 1.0592 - accuracy: 0.7067\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.2472 - accuracy: 0.9017\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.2012 - accuracy: 0.9183\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0852 - accuracy: 0.9750\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0967 - accuracy: 0.9750\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.1062 - accuracy: 0.9650\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0790 - accuracy: 0.9700\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0690 - accuracy: 0.9783\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0809 - accuracy: 0.9750\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0561 - accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7c55d4ca1e10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_model.fit(X_train, Y_train, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that completes, just use `.evaluate()` to evaluate against your test set. This function will print the value of the loss function and the performance metrics specified during the compilation of the model. In this case, the `binary_crossentropy` and the `accuracy` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 34ms/step - loss: 0.8148 - accuracy: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.814799964427948, 0.6800000071525574]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, right? But what if you need to build a model with shared layers, branches, or multiple inputs and outputs? This is where Sequential, with its beautifully simple yet limited functionality, won't be able to help you. \n",
    "\n",
    "Next up: Enter the Functional API, your slightly more complex, highly flexible friend.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - The Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second half of the assignment, where you'll use Keras' flexible [Functional API](https://www.tensorflow.org/guide/keras/functional) to build a ConvNet that can differentiate between 6 sign language digits. \n",
    "\n",
    "The Functional API can handle models with non-linear topology, shared layers, as well as layers with multiple inputs or outputs. Imagine that, where the Sequential API requires the model to move in a linear fashion through its layers, the Functional API allows much more flexibility. Where Sequential is a straight line, a Functional model is a graph, where the nodes of the layers can connect in many more ways than one. \n",
    "\n",
    "In the visual example below, the one possible direction of the movement Sequential model is shown in contrast to a skip connection, which is just one of the many ways a Functional model can be constructed. A skip connection, as you might have guessed, skips some layer in the network and feeds the output to a later layer in the network. Don't worry, you'll be spending more time with skip connections very soon! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/seq_vs_func.png\" style=\"width:350px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 - Load the SIGNS Dataset\n",
    "\n",
    "As a reminder, the SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (signs)\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_signs_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/SIGNS.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of `index` below and re-run to see different examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of an image from the dataset\n",
    "index = 9\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Split the Data into Train/Test Sets\n",
    "\n",
    "In Course 2, you built a fully-connected network for this dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it.\n",
    "\n",
    "To get started, let's examine the shapes of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Forward Propagation\n",
    "\n",
    "In TensorFlow, there are built-in functions that implement the convolution steps for you. By now, you should be familiar with how TensorFlow builds computational graphs. In the [Functional API](https://www.tensorflow.org/guide/keras/functional), you create a graph of layers. This is what allows such great flexibility.\n",
    "\n",
    "However, the following model could also be defined using the Sequential API since the information flow is on a single line. But don't deviate. What we want you to learn is to use the functional API.\n",
    "\n",
    "Begin building your graph of layers by creating an input node that functions as a callable object:\n",
    "\n",
    "- **input_img = tf.keras.Input(shape=input_shape):** \n",
    "\n",
    "Then, create a new node in the graph of layers by calling a layer on the `input_img` object: \n",
    "\n",
    "- **tf.keras.layers.Conv2D(filters= ... , kernel_size= ... , padding='same')(input_img):** Read the full documentation on [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).\n",
    "\n",
    "- **tf.keras.layers.MaxPool2D(pool_size=(f, f), strides=(s, s), padding='same'):** `MaxPool2D()` downsamples your input using a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window.  For max pooling, you usually operate on a single example at a time and a single channel at a time. Read the full documentation on [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D).\n",
    "\n",
    "- **tf.keras.layers.ReLU():** computes the elementwise ReLU of Z (which can be any shape). You can read the full documentation on [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU).\n",
    "\n",
    "- **tf.keras.layers.Flatten()**: given a tensor \"P\", this function takes each training (or test) example in the batch and flattens it into a 1D vector.  \n",
    "\n",
    "    * If a tensor P has the shape (batch_size,h,w,c), it returns a flattened tensor with shape (batch_size, k), where $k=h \\times w \\times c$.  \"k\" equals the product of all the dimension sizes other than the first dimension.\n",
    "    \n",
    "    * For example, given a tensor with dimensions [100, 2, 3, 4], it flattens the tensor to be of shape [100, 24], where 24 = 2 * 3 * 4.  You can read the full documentation on [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten).\n",
    "\n",
    "- **tf.keras.layers.Dense(units= ... , activation='softmax')(F):** given the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation on [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense).\n",
    "\n",
    "In the last function above (`tf.keras.layers.Dense()`), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters.\n",
    "\n",
    "Lastly, before creating the model, you'll need to define the output using the last of the function's compositions (in this example, a Dense layer): \n",
    "\n",
    "- **outputs = tf.keras.layers.Dense(units=6, activation='softmax')(F)**\n",
    "\n",
    "\n",
    "#### Window, kernel, filter, pool\n",
    "\n",
    "The words \"kernel\" and \"filter\" are used to refer to the same thing. The word \"filter\" accounts for the amount of \"kernels\" that will be used in a single convolution layer. \"Pool\" is the name of the operation that takes the max or average value of the kernels. \n",
    "\n",
    "This is why the parameter `pool_size` refers to `kernel_size`, and you use `(f,f)` to refer to the filter size. \n",
    "\n",
    "Pool size and kernel size refer to the same thing in different objects - They refer to the shape of the window where the operation takes place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - convolutional_model\n",
    "\n",
    "Implement the `convolutional_model` function below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE`. Use the functions above! \n",
    "\n",
    "Also, plug in the following parameters for all the steps:\n",
    "\n",
    " - [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D): Use 8 4 by 4 filters, stride 1, padding is \"SAME\"\n",
    " - [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)\n",
    " - [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D): Use an 8 by 8 filter size and an 8 by 8 stride, padding is \"SAME\"\n",
    " - **Conv2D**: Use 16 2 by 2 filters, stride 1, padding is \"SAME\"\n",
    " - **ReLU**\n",
    " - **MaxPool2D**: Use a 4 by 4 filter size and a 4 by 4 stride, padding is \"SAME\"\n",
    " - [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) the previous output.\n",
    " - Fully-connected ([Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer: Apply a fully connected layer with 6 neurons and a softmax activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f58643806aa8380c96225fc8b4c5e7aa",
     "grade": false,
     "grade_id": "cell-dac51744a9e03f51",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: convolutional_model\n",
    "\n",
    "def convolutional_model(input_shape):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE\n",
    "    \n",
    "    Note that for simplicity and grading purposes, you'll hard-code some values\n",
    "    such as the stride and kernel (filter) sizes. \n",
    "    Normally, functions should take these values as function parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    input_img -- input dataset, of shape (input_shape)\n",
    "\n",
    "    Returns:\n",
    "    model -- TF Keras model (object containing the information for the entire training process) \n",
    "    \"\"\"\n",
    "\n",
    "    input_img = tf.keras.Input(shape=input_shape)\n",
    "    Z1 = tf.keras.layers.Conv2D(8, (4, 4), strides=(1, 1), padding='SAME')(input_img)\n",
    "\n",
    "    A1 = tf.keras.layers.ReLU()(Z1)\n",
    "    \n",
    "    P1 = tf.keras.layers.MaxPooling2D(pool_size=(8, 8), strides=(8, 8), padding='SAME')(A1)\n",
    "\n",
    "    Z2 = tf.keras.layers.Conv2D(16, (2, 2), strides=(1, 1), padding='SAME')(P1)\n",
    "\n",
    "    A2 = tf.keras.layers.ReLU()(Z2)\n",
    "\n",
    "    P2 = tf.keras.layers.MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='SAME')(A2)\n",
    "\n",
    "    F = tf.keras.layers.Flatten()(P2)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(6, activation='softmax')(F)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_img, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "483d626949930a0b0ef20997e7c6ba72",
     "grade": true,
     "grade_id": "cell-45d22e92042174c9",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 8)         392       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 16)          528       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,310\n",
      "Trainable params: 1,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\u001b[32mAll tests passed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "conv_model = convolutional_model((64, 64, 3))\n",
    "conv_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "conv_model.summary()\n",
    "    \n",
    "output = [['InputLayer', [(None, 64, 64, 3)], 0],\n",
    "        ['Conv2D', (None, 64, 64, 8), 392, 'same', 'linear', 'GlorotUniform'],\n",
    "        ['ReLU', (None, 64, 64, 8), 0],\n",
    "        ['MaxPooling2D', (None, 8, 8, 8), 0, (8, 8), (8, 8), 'same'],\n",
    "        ['Conv2D', (None, 8, 8, 16), 528, 'same', 'linear', 'GlorotUniform'],\n",
    "        ['ReLU', (None, 8, 8, 16), 0],\n",
    "        ['MaxPooling2D', (None, 2, 2, 16), 0, (4, 4), (4, 4), 'same'],\n",
    "        ['Flatten', (None, 64), 0],\n",
    "        ['Dense', (None, 6), 390, 'softmax']]\n",
    "    \n",
    "comparator(summary(conv_model), output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the Sequential and Functional APIs return a TF Keras model object. The only difference is how inputs are handled inside the object model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-4'></a>\n",
    "### 4.4 - Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3598 - accuracy: 0.8889 - val_loss: 0.4373 - val_accuracy: 0.8167\n",
      "Epoch 2/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3574 - accuracy: 0.8917 - val_loss: 0.4352 - val_accuracy: 0.8167\n",
      "Epoch 3/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3547 - accuracy: 0.8935 - val_loss: 0.4340 - val_accuracy: 0.8250\n",
      "Epoch 4/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3527 - accuracy: 0.8935 - val_loss: 0.4330 - val_accuracy: 0.8250\n",
      "Epoch 5/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3497 - accuracy: 0.8944 - val_loss: 0.4314 - val_accuracy: 0.8250\n",
      "Epoch 6/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3475 - accuracy: 0.8944 - val_loss: 0.4297 - val_accuracy: 0.8250\n",
      "Epoch 7/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3451 - accuracy: 0.8944 - val_loss: 0.4286 - val_accuracy: 0.8250\n",
      "Epoch 8/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3431 - accuracy: 0.8963 - val_loss: 0.4276 - val_accuracy: 0.8333\n",
      "Epoch 9/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.3402 - accuracy: 0.8963 - val_loss: 0.4255 - val_accuracy: 0.8333\n",
      "Epoch 10/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3382 - accuracy: 0.8972 - val_loss: 0.4246 - val_accuracy: 0.8417\n",
      "Epoch 11/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3360 - accuracy: 0.8972 - val_loss: 0.4237 - val_accuracy: 0.8333\n",
      "Epoch 12/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3338 - accuracy: 0.8991 - val_loss: 0.4222 - val_accuracy: 0.8333\n",
      "Epoch 13/250\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 0.3313 - accuracy: 0.8991 - val_loss: 0.4209 - val_accuracy: 0.8333\n",
      "Epoch 14/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3293 - accuracy: 0.9009 - val_loss: 0.4198 - val_accuracy: 0.8417\n",
      "Epoch 15/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3271 - accuracy: 0.9037 - val_loss: 0.4190 - val_accuracy: 0.8333\n",
      "Epoch 16/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3244 - accuracy: 0.9046 - val_loss: 0.4170 - val_accuracy: 0.8417\n",
      "Epoch 17/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3225 - accuracy: 0.9074 - val_loss: 0.4165 - val_accuracy: 0.8417\n",
      "Epoch 18/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3203 - accuracy: 0.9074 - val_loss: 0.4151 - val_accuracy: 0.8417\n",
      "Epoch 19/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3184 - accuracy: 0.9074 - val_loss: 0.4137 - val_accuracy: 0.8417\n",
      "Epoch 20/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3162 - accuracy: 0.9074 - val_loss: 0.4133 - val_accuracy: 0.8417\n",
      "Epoch 21/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3143 - accuracy: 0.9074 - val_loss: 0.4128 - val_accuracy: 0.8417\n",
      "Epoch 22/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3118 - accuracy: 0.9093 - val_loss: 0.4114 - val_accuracy: 0.8417\n",
      "Epoch 23/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3103 - accuracy: 0.9102 - val_loss: 0.4107 - val_accuracy: 0.8417\n",
      "Epoch 24/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3080 - accuracy: 0.9111 - val_loss: 0.4098 - val_accuracy: 0.8417\n",
      "Epoch 25/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3062 - accuracy: 0.9102 - val_loss: 0.4083 - val_accuracy: 0.8417\n",
      "Epoch 26/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3043 - accuracy: 0.9111 - val_loss: 0.4079 - val_accuracy: 0.8417\n",
      "Epoch 27/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.3021 - accuracy: 0.9120 - val_loss: 0.4069 - val_accuracy: 0.8417\n",
      "Epoch 28/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3004 - accuracy: 0.9130 - val_loss: 0.4063 - val_accuracy: 0.8417\n",
      "Epoch 29/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2984 - accuracy: 0.9139 - val_loss: 0.4054 - val_accuracy: 0.8417\n",
      "Epoch 30/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2967 - accuracy: 0.9148 - val_loss: 0.4044 - val_accuracy: 0.8417\n",
      "Epoch 31/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2950 - accuracy: 0.9139 - val_loss: 0.4044 - val_accuracy: 0.8417\n",
      "Epoch 32/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2928 - accuracy: 0.9167 - val_loss: 0.4030 - val_accuracy: 0.8417\n",
      "Epoch 33/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2912 - accuracy: 0.9157 - val_loss: 0.4031 - val_accuracy: 0.8417\n",
      "Epoch 34/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2892 - accuracy: 0.9167 - val_loss: 0.4016 - val_accuracy: 0.8500\n",
      "Epoch 35/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2875 - accuracy: 0.9157 - val_loss: 0.4012 - val_accuracy: 0.8417\n",
      "Epoch 36/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2860 - accuracy: 0.9167 - val_loss: 0.4010 - val_accuracy: 0.8417\n",
      "Epoch 37/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2839 - accuracy: 0.9167 - val_loss: 0.4002 - val_accuracy: 0.8417\n",
      "Epoch 38/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2821 - accuracy: 0.9157 - val_loss: 0.4001 - val_accuracy: 0.8417\n",
      "Epoch 39/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2804 - accuracy: 0.9157 - val_loss: 0.3995 - val_accuracy: 0.8417\n",
      "Epoch 40/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2787 - accuracy: 0.9167 - val_loss: 0.3980 - val_accuracy: 0.8500\n",
      "Epoch 41/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2771 - accuracy: 0.9167 - val_loss: 0.3984 - val_accuracy: 0.8417\n",
      "Epoch 42/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2755 - accuracy: 0.9167 - val_loss: 0.3975 - val_accuracy: 0.8417\n",
      "Epoch 43/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2738 - accuracy: 0.9167 - val_loss: 0.3972 - val_accuracy: 0.8417\n",
      "Epoch 44/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2723 - accuracy: 0.9167 - val_loss: 0.3971 - val_accuracy: 0.8417\n",
      "Epoch 45/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2707 - accuracy: 0.9157 - val_loss: 0.3959 - val_accuracy: 0.8500\n",
      "Epoch 46/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2692 - accuracy: 0.9185 - val_loss: 0.3965 - val_accuracy: 0.8417\n",
      "Epoch 47/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2674 - accuracy: 0.9185 - val_loss: 0.3958 - val_accuracy: 0.8417\n",
      "Epoch 48/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2658 - accuracy: 0.9194 - val_loss: 0.3954 - val_accuracy: 0.8417\n",
      "Epoch 49/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2642 - accuracy: 0.9204 - val_loss: 0.3948 - val_accuracy: 0.8500\n",
      "Epoch 50/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2626 - accuracy: 0.9213 - val_loss: 0.3942 - val_accuracy: 0.8417\n",
      "Epoch 51/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2612 - accuracy: 0.9222 - val_loss: 0.3943 - val_accuracy: 0.8417\n",
      "Epoch 52/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2596 - accuracy: 0.9222 - val_loss: 0.3940 - val_accuracy: 0.8417\n",
      "Epoch 53/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2583 - accuracy: 0.9222 - val_loss: 0.3937 - val_accuracy: 0.8583\n",
      "Epoch 54/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2565 - accuracy: 0.9231 - val_loss: 0.3938 - val_accuracy: 0.8500\n",
      "Epoch 55/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2553 - accuracy: 0.9231 - val_loss: 0.3930 - val_accuracy: 0.8583\n",
      "Epoch 56/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2537 - accuracy: 0.9231 - val_loss: 0.3924 - val_accuracy: 0.8583\n",
      "Epoch 57/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2525 - accuracy: 0.9250 - val_loss: 0.3917 - val_accuracy: 0.8583\n",
      "Epoch 58/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2509 - accuracy: 0.9250 - val_loss: 0.3918 - val_accuracy: 0.8583\n",
      "Epoch 59/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2495 - accuracy: 0.9259 - val_loss: 0.3915 - val_accuracy: 0.8583\n",
      "Epoch 60/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2479 - accuracy: 0.9241 - val_loss: 0.3920 - val_accuracy: 0.8583\n",
      "Epoch 61/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2467 - accuracy: 0.9259 - val_loss: 0.3918 - val_accuracy: 0.8583\n",
      "Epoch 62/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2450 - accuracy: 0.9278 - val_loss: 0.3914 - val_accuracy: 0.8667\n",
      "Epoch 63/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2438 - accuracy: 0.9269 - val_loss: 0.3910 - val_accuracy: 0.8667\n",
      "Epoch 64/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2424 - accuracy: 0.9287 - val_loss: 0.3907 - val_accuracy: 0.8667\n",
      "Epoch 65/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2409 - accuracy: 0.9296 - val_loss: 0.3905 - val_accuracy: 0.8667\n",
      "Epoch 66/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2395 - accuracy: 0.9296 - val_loss: 0.3899 - val_accuracy: 0.8750\n",
      "Epoch 67/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2381 - accuracy: 0.9296 - val_loss: 0.3895 - val_accuracy: 0.8750\n",
      "Epoch 68/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2369 - accuracy: 0.9306 - val_loss: 0.3891 - val_accuracy: 0.8750\n",
      "Epoch 69/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2355 - accuracy: 0.9306 - val_loss: 0.3890 - val_accuracy: 0.8750\n",
      "Epoch 70/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2343 - accuracy: 0.9315 - val_loss: 0.3890 - val_accuracy: 0.8750\n",
      "Epoch 71/250\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.2329 - accuracy: 0.9315 - val_loss: 0.3889 - val_accuracy: 0.8750\n",
      "Epoch 72/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2315 - accuracy: 0.9315 - val_loss: 0.3881 - val_accuracy: 0.8750\n",
      "Epoch 73/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2306 - accuracy: 0.9315 - val_loss: 0.3881 - val_accuracy: 0.8750\n",
      "Epoch 74/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2290 - accuracy: 0.9315 - val_loss: 0.3878 - val_accuracy: 0.8750\n",
      "Epoch 75/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2280 - accuracy: 0.9324 - val_loss: 0.3873 - val_accuracy: 0.8750\n",
      "Epoch 76/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2265 - accuracy: 0.9333 - val_loss: 0.3870 - val_accuracy: 0.8750\n",
      "Epoch 77/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2252 - accuracy: 0.9352 - val_loss: 0.3867 - val_accuracy: 0.8750\n",
      "Epoch 78/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2239 - accuracy: 0.9333 - val_loss: 0.3869 - val_accuracy: 0.8750\n",
      "Epoch 79/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2227 - accuracy: 0.9352 - val_loss: 0.3857 - val_accuracy: 0.8750\n",
      "Epoch 80/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2214 - accuracy: 0.9343 - val_loss: 0.3859 - val_accuracy: 0.8750\n",
      "Epoch 81/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2202 - accuracy: 0.9352 - val_loss: 0.3865 - val_accuracy: 0.8750\n",
      "Epoch 82/250\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.2190 - accuracy: 0.9361 - val_loss: 0.3860 - val_accuracy: 0.8750\n",
      "Epoch 83/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2180 - accuracy: 0.9370 - val_loss: 0.3858 - val_accuracy: 0.8750\n",
      "Epoch 84/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2167 - accuracy: 0.9370 - val_loss: 0.3858 - val_accuracy: 0.8750\n",
      "Epoch 85/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2155 - accuracy: 0.9370 - val_loss: 0.3849 - val_accuracy: 0.8750\n",
      "Epoch 86/250\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.2145 - accuracy: 0.9370 - val_loss: 0.3852 - val_accuracy: 0.8750\n",
      "Epoch 87/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2132 - accuracy: 0.9361 - val_loss: 0.3853 - val_accuracy: 0.8750\n",
      "Epoch 88/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2120 - accuracy: 0.9370 - val_loss: 0.3850 - val_accuracy: 0.8750\n",
      "Epoch 89/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2108 - accuracy: 0.9380 - val_loss: 0.3847 - val_accuracy: 0.8750\n",
      "Epoch 90/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2094 - accuracy: 0.9398 - val_loss: 0.3837 - val_accuracy: 0.8750\n",
      "Epoch 91/250\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 0.2084 - accuracy: 0.9398 - val_loss: 0.3849 - val_accuracy: 0.8750\n",
      "Epoch 92/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2070 - accuracy: 0.9417 - val_loss: 0.3844 - val_accuracy: 0.8750\n",
      "Epoch 93/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2061 - accuracy: 0.9417 - val_loss: 0.3844 - val_accuracy: 0.8750\n",
      "Epoch 94/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2048 - accuracy: 0.9435 - val_loss: 0.3835 - val_accuracy: 0.8750\n",
      "Epoch 95/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2032 - accuracy: 0.9417 - val_loss: 0.3849 - val_accuracy: 0.8667\n",
      "Epoch 96/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2020 - accuracy: 0.9444 - val_loss: 0.3845 - val_accuracy: 0.8583\n",
      "Epoch 97/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2011 - accuracy: 0.9444 - val_loss: 0.3853 - val_accuracy: 0.8583\n",
      "Epoch 98/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1998 - accuracy: 0.9444 - val_loss: 0.3846 - val_accuracy: 0.8583\n",
      "Epoch 99/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1987 - accuracy: 0.9444 - val_loss: 0.3854 - val_accuracy: 0.8583\n",
      "Epoch 100/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1973 - accuracy: 0.9454 - val_loss: 0.3848 - val_accuracy: 0.8583\n",
      "Epoch 101/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1963 - accuracy: 0.9463 - val_loss: 0.3858 - val_accuracy: 0.8583\n",
      "Epoch 102/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1952 - accuracy: 0.9463 - val_loss: 0.3846 - val_accuracy: 0.8583\n",
      "Epoch 103/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1939 - accuracy: 0.9463 - val_loss: 0.3857 - val_accuracy: 0.8583\n",
      "Epoch 104/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1927 - accuracy: 0.9472 - val_loss: 0.3852 - val_accuracy: 0.8583\n",
      "Epoch 105/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1918 - accuracy: 0.9481 - val_loss: 0.3860 - val_accuracy: 0.8583\n",
      "Epoch 106/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1907 - accuracy: 0.9491 - val_loss: 0.3861 - val_accuracy: 0.8583\n",
      "Epoch 107/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1896 - accuracy: 0.9481 - val_loss: 0.3861 - val_accuracy: 0.8583\n",
      "Epoch 108/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1886 - accuracy: 0.9491 - val_loss: 0.3869 - val_accuracy: 0.8583\n",
      "Epoch 109/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1876 - accuracy: 0.9500 - val_loss: 0.3875 - val_accuracy: 0.8583\n",
      "Epoch 110/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1866 - accuracy: 0.9509 - val_loss: 0.3871 - val_accuracy: 0.8583\n",
      "Epoch 111/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1856 - accuracy: 0.9509 - val_loss: 0.3870 - val_accuracy: 0.8583\n",
      "Epoch 112/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1843 - accuracy: 0.9509 - val_loss: 0.3873 - val_accuracy: 0.8583\n",
      "Epoch 113/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1835 - accuracy: 0.9519 - val_loss: 0.3874 - val_accuracy: 0.8583\n",
      "Epoch 114/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1825 - accuracy: 0.9509 - val_loss: 0.3872 - val_accuracy: 0.8583\n",
      "Epoch 115/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1813 - accuracy: 0.9509 - val_loss: 0.3879 - val_accuracy: 0.8583\n",
      "Epoch 116/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1802 - accuracy: 0.9519 - val_loss: 0.3880 - val_accuracy: 0.8583\n",
      "Epoch 117/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1795 - accuracy: 0.9519 - val_loss: 0.3874 - val_accuracy: 0.8583\n",
      "Epoch 118/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1783 - accuracy: 0.9519 - val_loss: 0.3880 - val_accuracy: 0.8583\n",
      "Epoch 119/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1773 - accuracy: 0.9519 - val_loss: 0.3885 - val_accuracy: 0.8583\n",
      "Epoch 120/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1765 - accuracy: 0.9537 - val_loss: 0.3873 - val_accuracy: 0.8583\n",
      "Epoch 121/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1755 - accuracy: 0.9546 - val_loss: 0.3884 - val_accuracy: 0.8583\n",
      "Epoch 122/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1744 - accuracy: 0.9546 - val_loss: 0.3874 - val_accuracy: 0.8583\n",
      "Epoch 123/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1736 - accuracy: 0.9546 - val_loss: 0.3886 - val_accuracy: 0.8583\n",
      "Epoch 124/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1728 - accuracy: 0.9556 - val_loss: 0.3897 - val_accuracy: 0.8583\n",
      "Epoch 125/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1720 - accuracy: 0.9556 - val_loss: 0.3893 - val_accuracy: 0.8583\n",
      "Epoch 126/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1711 - accuracy: 0.9556 - val_loss: 0.3902 - val_accuracy: 0.8583\n",
      "Epoch 127/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1702 - accuracy: 0.9556 - val_loss: 0.3902 - val_accuracy: 0.8583\n",
      "Epoch 128/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1693 - accuracy: 0.9565 - val_loss: 0.3896 - val_accuracy: 0.8583\n",
      "Epoch 129/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1687 - accuracy: 0.9565 - val_loss: 0.3902 - val_accuracy: 0.8583\n",
      "Epoch 130/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1679 - accuracy: 0.9565 - val_loss: 0.3910 - val_accuracy: 0.8583\n",
      "Epoch 131/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1671 - accuracy: 0.9574 - val_loss: 0.3914 - val_accuracy: 0.8583\n",
      "Epoch 132/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1664 - accuracy: 0.9574 - val_loss: 0.3914 - val_accuracy: 0.8583\n",
      "Epoch 133/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1658 - accuracy: 0.9574 - val_loss: 0.3918 - val_accuracy: 0.8583\n",
      "Epoch 134/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1649 - accuracy: 0.9574 - val_loss: 0.3926 - val_accuracy: 0.8583\n",
      "Epoch 135/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1639 - accuracy: 0.9574 - val_loss: 0.3925 - val_accuracy: 0.8583\n",
      "Epoch 136/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1631 - accuracy: 0.9574 - val_loss: 0.3937 - val_accuracy: 0.8583\n",
      "Epoch 137/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1625 - accuracy: 0.9583 - val_loss: 0.3933 - val_accuracy: 0.8583\n",
      "Epoch 138/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1619 - accuracy: 0.9583 - val_loss: 0.3941 - val_accuracy: 0.8583\n",
      "Epoch 139/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1611 - accuracy: 0.9593 - val_loss: 0.3942 - val_accuracy: 0.8583\n",
      "Epoch 140/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1600 - accuracy: 0.9593 - val_loss: 0.3944 - val_accuracy: 0.8583\n",
      "Epoch 141/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1596 - accuracy: 0.9593 - val_loss: 0.3950 - val_accuracy: 0.8583\n",
      "Epoch 142/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1588 - accuracy: 0.9602 - val_loss: 0.3953 - val_accuracy: 0.8583\n",
      "Epoch 143/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1582 - accuracy: 0.9602 - val_loss: 0.3971 - val_accuracy: 0.8583\n",
      "Epoch 144/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1574 - accuracy: 0.9602 - val_loss: 0.3970 - val_accuracy: 0.8583\n",
      "Epoch 145/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1568 - accuracy: 0.9602 - val_loss: 0.3973 - val_accuracy: 0.8583\n",
      "Epoch 146/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1561 - accuracy: 0.9602 - val_loss: 0.3976 - val_accuracy: 0.8583\n",
      "Epoch 147/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1556 - accuracy: 0.9602 - val_loss: 0.3984 - val_accuracy: 0.8417\n",
      "Epoch 148/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1546 - accuracy: 0.9602 - val_loss: 0.3993 - val_accuracy: 0.8417\n",
      "Epoch 149/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1539 - accuracy: 0.9611 - val_loss: 0.3991 - val_accuracy: 0.8417\n",
      "Epoch 150/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1532 - accuracy: 0.9611 - val_loss: 0.3991 - val_accuracy: 0.8417\n",
      "Epoch 151/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1524 - accuracy: 0.9620 - val_loss: 0.4004 - val_accuracy: 0.8417\n",
      "Epoch 152/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1518 - accuracy: 0.9630 - val_loss: 0.4007 - val_accuracy: 0.8417\n",
      "Epoch 153/250\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.1510 - accuracy: 0.9620 - val_loss: 0.4006 - val_accuracy: 0.8417\n",
      "Epoch 154/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1504 - accuracy: 0.9611 - val_loss: 0.4013 - val_accuracy: 0.8417\n",
      "Epoch 155/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1498 - accuracy: 0.9620 - val_loss: 0.4022 - val_accuracy: 0.8417\n",
      "Epoch 156/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1489 - accuracy: 0.9611 - val_loss: 0.4026 - val_accuracy: 0.8417\n",
      "Epoch 157/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1480 - accuracy: 0.9611 - val_loss: 0.4024 - val_accuracy: 0.8417\n",
      "Epoch 158/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1476 - accuracy: 0.9611 - val_loss: 0.4044 - val_accuracy: 0.8417\n",
      "Epoch 159/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1467 - accuracy: 0.9611 - val_loss: 0.4030 - val_accuracy: 0.8417\n",
      "Epoch 160/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1464 - accuracy: 0.9620 - val_loss: 0.4052 - val_accuracy: 0.8417\n",
      "Epoch 161/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1455 - accuracy: 0.9620 - val_loss: 0.4040 - val_accuracy: 0.8417\n",
      "Epoch 162/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1450 - accuracy: 0.9639 - val_loss: 0.4050 - val_accuracy: 0.8500\n",
      "Epoch 163/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1446 - accuracy: 0.9620 - val_loss: 0.4064 - val_accuracy: 0.8500\n",
      "Epoch 164/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1441 - accuracy: 0.9630 - val_loss: 0.4070 - val_accuracy: 0.8500\n",
      "Epoch 165/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1432 - accuracy: 0.9630 - val_loss: 0.4073 - val_accuracy: 0.8500\n",
      "Epoch 166/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1428 - accuracy: 0.9630 - val_loss: 0.4072 - val_accuracy: 0.8500\n",
      "Epoch 167/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1423 - accuracy: 0.9630 - val_loss: 0.4087 - val_accuracy: 0.8500\n",
      "Epoch 168/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1422 - accuracy: 0.9630 - val_loss: 0.4097 - val_accuracy: 0.8500\n",
      "Epoch 169/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1412 - accuracy: 0.9630 - val_loss: 0.4111 - val_accuracy: 0.8500\n",
      "Epoch 170/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1405 - accuracy: 0.9630 - val_loss: 0.4108 - val_accuracy: 0.8500\n",
      "Epoch 171/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1402 - accuracy: 0.9630 - val_loss: 0.4111 - val_accuracy: 0.8500\n",
      "Epoch 172/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1395 - accuracy: 0.9630 - val_loss: 0.4123 - val_accuracy: 0.8500\n",
      "Epoch 173/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1389 - accuracy: 0.9639 - val_loss: 0.4129 - val_accuracy: 0.8500\n",
      "Epoch 174/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1384 - accuracy: 0.9648 - val_loss: 0.4140 - val_accuracy: 0.8500\n",
      "Epoch 175/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1375 - accuracy: 0.9657 - val_loss: 0.4145 - val_accuracy: 0.8500\n",
      "Epoch 176/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1371 - accuracy: 0.9648 - val_loss: 0.4148 - val_accuracy: 0.8500\n",
      "Epoch 177/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1366 - accuracy: 0.9657 - val_loss: 0.4154 - val_accuracy: 0.8500\n",
      "Epoch 178/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1361 - accuracy: 0.9648 - val_loss: 0.4166 - val_accuracy: 0.8500\n",
      "Epoch 179/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1352 - accuracy: 0.9648 - val_loss: 0.4165 - val_accuracy: 0.8500\n",
      "Epoch 180/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1348 - accuracy: 0.9657 - val_loss: 0.4172 - val_accuracy: 0.8500\n",
      "Epoch 181/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1343 - accuracy: 0.9648 - val_loss: 0.4184 - val_accuracy: 0.8500\n",
      "Epoch 182/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1338 - accuracy: 0.9648 - val_loss: 0.4188 - val_accuracy: 0.8500\n",
      "Epoch 183/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1331 - accuracy: 0.9657 - val_loss: 0.4190 - val_accuracy: 0.8500\n",
      "Epoch 184/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1327 - accuracy: 0.9639 - val_loss: 0.4200 - val_accuracy: 0.8500\n",
      "Epoch 185/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1322 - accuracy: 0.9648 - val_loss: 0.4210 - val_accuracy: 0.8500\n",
      "Epoch 186/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1315 - accuracy: 0.9639 - val_loss: 0.4207 - val_accuracy: 0.8583\n",
      "Epoch 187/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1309 - accuracy: 0.9648 - val_loss: 0.4215 - val_accuracy: 0.8500\n",
      "Epoch 188/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1306 - accuracy: 0.9657 - val_loss: 0.4227 - val_accuracy: 0.8500\n",
      "Epoch 189/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1301 - accuracy: 0.9667 - val_loss: 0.4239 - val_accuracy: 0.8500\n",
      "Epoch 190/250\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.1295 - accuracy: 0.9657 - val_loss: 0.4243 - val_accuracy: 0.8583\n",
      "Epoch 191/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1287 - accuracy: 0.9657 - val_loss: 0.4247 - val_accuracy: 0.8583\n",
      "Epoch 192/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1281 - accuracy: 0.9657 - val_loss: 0.4245 - val_accuracy: 0.8583\n",
      "Epoch 193/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1278 - accuracy: 0.9657 - val_loss: 0.4261 - val_accuracy: 0.8583\n",
      "Epoch 194/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1274 - accuracy: 0.9657 - val_loss: 0.4265 - val_accuracy: 0.8583\n",
      "Epoch 195/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1269 - accuracy: 0.9667 - val_loss: 0.4274 - val_accuracy: 0.8583\n",
      "Epoch 196/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1264 - accuracy: 0.9667 - val_loss: 0.4279 - val_accuracy: 0.8583\n",
      "Epoch 197/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1259 - accuracy: 0.9676 - val_loss: 0.4281 - val_accuracy: 0.8583\n",
      "Epoch 198/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1256 - accuracy: 0.9676 - val_loss: 0.4286 - val_accuracy: 0.8583\n",
      "Epoch 199/250\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.1249 - accuracy: 0.9676 - val_loss: 0.4292 - val_accuracy: 0.8583\n",
      "Epoch 200/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1244 - accuracy: 0.9676 - val_loss: 0.4307 - val_accuracy: 0.8583\n",
      "Epoch 201/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1239 - accuracy: 0.9676 - val_loss: 0.4298 - val_accuracy: 0.8583\n",
      "Epoch 202/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1236 - accuracy: 0.9685 - val_loss: 0.4320 - val_accuracy: 0.8583\n",
      "Epoch 203/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1231 - accuracy: 0.9685 - val_loss: 0.4324 - val_accuracy: 0.8583\n",
      "Epoch 204/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1224 - accuracy: 0.9685 - val_loss: 0.4324 - val_accuracy: 0.8583\n",
      "Epoch 205/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1218 - accuracy: 0.9685 - val_loss: 0.4331 - val_accuracy: 0.8583\n",
      "Epoch 206/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1213 - accuracy: 0.9685 - val_loss: 0.4340 - val_accuracy: 0.8583\n",
      "Epoch 207/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1209 - accuracy: 0.9685 - val_loss: 0.4344 - val_accuracy: 0.8583\n",
      "Epoch 208/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1206 - accuracy: 0.9685 - val_loss: 0.4348 - val_accuracy: 0.8583\n",
      "Epoch 209/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1203 - accuracy: 0.9685 - val_loss: 0.4367 - val_accuracy: 0.8583\n",
      "Epoch 210/250\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.1197 - accuracy: 0.9685 - val_loss: 0.4364 - val_accuracy: 0.8583\n",
      "Epoch 211/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1191 - accuracy: 0.9685 - val_loss: 0.4375 - val_accuracy: 0.8583\n",
      "Epoch 212/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1188 - accuracy: 0.9685 - val_loss: 0.4383 - val_accuracy: 0.8583\n",
      "Epoch 213/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1184 - accuracy: 0.9676 - val_loss: 0.4384 - val_accuracy: 0.8583\n",
      "Epoch 214/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1179 - accuracy: 0.9676 - val_loss: 0.4383 - val_accuracy: 0.8583\n",
      "Epoch 215/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1177 - accuracy: 0.9676 - val_loss: 0.4399 - val_accuracy: 0.8583\n",
      "Epoch 216/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1172 - accuracy: 0.9676 - val_loss: 0.4411 - val_accuracy: 0.8500\n",
      "Epoch 217/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1163 - accuracy: 0.9676 - val_loss: 0.4405 - val_accuracy: 0.8500\n",
      "Epoch 218/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1154 - accuracy: 0.9667 - val_loss: 0.4395 - val_accuracy: 0.8583\n",
      "Epoch 219/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1152 - accuracy: 0.9676 - val_loss: 0.4390 - val_accuracy: 0.8500\n",
      "Epoch 220/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1147 - accuracy: 0.9685 - val_loss: 0.4418 - val_accuracy: 0.8583\n",
      "Epoch 221/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1140 - accuracy: 0.9685 - val_loss: 0.4417 - val_accuracy: 0.8583\n",
      "Epoch 222/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1137 - accuracy: 0.9685 - val_loss: 0.4413 - val_accuracy: 0.8583\n",
      "Epoch 223/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1134 - accuracy: 0.9694 - val_loss: 0.4436 - val_accuracy: 0.8583\n",
      "Epoch 224/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1127 - accuracy: 0.9685 - val_loss: 0.4409 - val_accuracy: 0.8583\n",
      "Epoch 225/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1122 - accuracy: 0.9694 - val_loss: 0.4423 - val_accuracy: 0.8583\n",
      "Epoch 226/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1117 - accuracy: 0.9694 - val_loss: 0.4425 - val_accuracy: 0.8583\n",
      "Epoch 227/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1110 - accuracy: 0.9694 - val_loss: 0.4419 - val_accuracy: 0.8583\n",
      "Epoch 228/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1103 - accuracy: 0.9694 - val_loss: 0.4419 - val_accuracy: 0.8583\n",
      "Epoch 229/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1096 - accuracy: 0.9694 - val_loss: 0.4424 - val_accuracy: 0.8583\n",
      "Epoch 230/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1091 - accuracy: 0.9704 - val_loss: 0.4419 - val_accuracy: 0.8583\n",
      "Epoch 231/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1087 - accuracy: 0.9694 - val_loss: 0.4424 - val_accuracy: 0.8583\n",
      "Epoch 232/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1080 - accuracy: 0.9713 - val_loss: 0.4417 - val_accuracy: 0.8583\n",
      "Epoch 233/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1074 - accuracy: 0.9722 - val_loss: 0.4426 - val_accuracy: 0.8583\n",
      "Epoch 234/250\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1067 - accuracy: 0.9722 - val_loss: 0.4414 - val_accuracy: 0.8583\n",
      "Epoch 235/250\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1061 - accuracy: 0.9731 - val_loss: 0.4413 - val_accuracy: 0.8583\n",
      "Epoch 236/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1057 - accuracy: 0.9731 - val_loss: 0.4448 - val_accuracy: 0.8583\n",
      "Epoch 237/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1048 - accuracy: 0.9731 - val_loss: 0.4407 - val_accuracy: 0.8583\n",
      "Epoch 238/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1044 - accuracy: 0.9731 - val_loss: 0.4421 - val_accuracy: 0.8583\n",
      "Epoch 239/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1037 - accuracy: 0.9731 - val_loss: 0.4436 - val_accuracy: 0.8583\n",
      "Epoch 240/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1031 - accuracy: 0.9731 - val_loss: 0.4414 - val_accuracy: 0.8583\n",
      "Epoch 241/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1026 - accuracy: 0.9750 - val_loss: 0.4435 - val_accuracy: 0.8583\n",
      "Epoch 242/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1019 - accuracy: 0.9759 - val_loss: 0.4435 - val_accuracy: 0.8583\n",
      "Epoch 243/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1011 - accuracy: 0.9759 - val_loss: 0.4415 - val_accuracy: 0.8583\n",
      "Epoch 244/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1008 - accuracy: 0.9759 - val_loss: 0.4434 - val_accuracy: 0.8583\n",
      "Epoch 245/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0999 - accuracy: 0.9769 - val_loss: 0.4417 - val_accuracy: 0.8583\n",
      "Epoch 246/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0994 - accuracy: 0.9769 - val_loss: 0.4441 - val_accuracy: 0.8583\n",
      "Epoch 247/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0987 - accuracy: 0.9769 - val_loss: 0.4418 - val_accuracy: 0.8583\n",
      "Epoch 248/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0982 - accuracy: 0.9769 - val_loss: 0.4434 - val_accuracy: 0.8583\n",
      "Epoch 249/250\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0977 - accuracy: 0.9769 - val_loss: 0.4429 - val_accuracy: 0.8583\n",
      "Epoch 250/250\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0971 - accuracy: 0.9778 - val_loss: 0.4444 - val_accuracy: 0.8583\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(64)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(64)\n",
    "history = conv_model.fit(train_dataset, epochs=250, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - History Object \n",
    "\n",
    "The history object is an output of the `.fit()` operation, and provides a record of all the loss and metric values in memory. It's stored as a dictionary that you can retrieve at `history.history`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.35982418060302734,\n",
       "  0.35742586851119995,\n",
       "  0.3547055721282959,\n",
       "  0.3526594042778015,\n",
       "  0.34972184896469116,\n",
       "  0.34751173853874207,\n",
       "  0.34513458609580994,\n",
       "  0.3431006968021393,\n",
       "  0.34018656611442566,\n",
       "  0.3382129669189453,\n",
       "  0.3360039293766022,\n",
       "  0.33376485109329224,\n",
       "  0.3312637507915497,\n",
       "  0.3293297290802002,\n",
       "  0.3271041214466095,\n",
       "  0.32437580823898315,\n",
       "  0.3224872052669525,\n",
       "  0.3202633857727051,\n",
       "  0.31838664412498474,\n",
       "  0.31624963879585266,\n",
       "  0.31431302428245544,\n",
       "  0.31183922290802,\n",
       "  0.3103204071521759,\n",
       "  0.307950884103775,\n",
       "  0.3062126040458679,\n",
       "  0.30426841974258423,\n",
       "  0.30209001898765564,\n",
       "  0.3003971576690674,\n",
       "  0.29843276739120483,\n",
       "  0.29674190282821655,\n",
       "  0.29504650831222534,\n",
       "  0.2927888333797455,\n",
       "  0.2911539077758789,\n",
       "  0.2892496883869171,\n",
       "  0.2875271439552307,\n",
       "  0.28597283363342285,\n",
       "  0.28387588262557983,\n",
       "  0.2821161150932312,\n",
       "  0.2803557515144348,\n",
       "  0.2786925137042999,\n",
       "  0.27706244587898254,\n",
       "  0.27553847432136536,\n",
       "  0.2737538516521454,\n",
       "  0.27234458923339844,\n",
       "  0.2707330286502838,\n",
       "  0.26915228366851807,\n",
       "  0.2674131393432617,\n",
       "  0.2658214867115021,\n",
       "  0.2641630172729492,\n",
       "  0.26262837648391724,\n",
       "  0.26122915744781494,\n",
       "  0.2596147060394287,\n",
       "  0.25826379656791687,\n",
       "  0.25654128193855286,\n",
       "  0.25532442331314087,\n",
       "  0.25370386242866516,\n",
       "  0.25246861577033997,\n",
       "  0.2509024739265442,\n",
       "  0.24947142601013184,\n",
       "  0.24789977073669434,\n",
       "  0.24674423038959503,\n",
       "  0.2450469732284546,\n",
       "  0.24376051127910614,\n",
       "  0.24244195222854614,\n",
       "  0.2408653050661087,\n",
       "  0.23952603340148926,\n",
       "  0.2380996197462082,\n",
       "  0.23690423369407654,\n",
       "  0.23548556864261627,\n",
       "  0.23429161310195923,\n",
       "  0.23289397358894348,\n",
       "  0.23151174187660217,\n",
       "  0.2305627018213272,\n",
       "  0.2289758026599884,\n",
       "  0.22797004878520966,\n",
       "  0.22645044326782227,\n",
       "  0.2251855880022049,\n",
       "  0.2238530069589615,\n",
       "  0.222744420170784,\n",
       "  0.22144563496112823,\n",
       "  0.22020213305950165,\n",
       "  0.21904833614826202,\n",
       "  0.21795733273029327,\n",
       "  0.21668493747711182,\n",
       "  0.2154615968465805,\n",
       "  0.21454386413097382,\n",
       "  0.21316760778427124,\n",
       "  0.2120450884103775,\n",
       "  0.21082133054733276,\n",
       "  0.20941241085529327,\n",
       "  0.20837095379829407,\n",
       "  0.2070164978504181,\n",
       "  0.20614440739154816,\n",
       "  0.20475219190120697,\n",
       "  0.20320044457912445,\n",
       "  0.20203295350074768,\n",
       "  0.2010931819677353,\n",
       "  0.1997898519039154,\n",
       "  0.19871507585048676,\n",
       "  0.1972733736038208,\n",
       "  0.19631023705005646,\n",
       "  0.19518429040908813,\n",
       "  0.19386795163154602,\n",
       "  0.19268685579299927,\n",
       "  0.19175627827644348,\n",
       "  0.19068308174610138,\n",
       "  0.18955069780349731,\n",
       "  0.18860672414302826,\n",
       "  0.1875634342432022,\n",
       "  0.18659715354442596,\n",
       "  0.18557427823543549,\n",
       "  0.18425901234149933,\n",
       "  0.1834641844034195,\n",
       "  0.18247853219509125,\n",
       "  0.18129591643810272,\n",
       "  0.18018169701099396,\n",
       "  0.1794576197862625,\n",
       "  0.1783183068037033,\n",
       "  0.17730630934238434,\n",
       "  0.17647922039031982,\n",
       "  0.17550545930862427,\n",
       "  0.17443983256816864,\n",
       "  0.17364569008350372,\n",
       "  0.17278710007667542,\n",
       "  0.17201516032218933,\n",
       "  0.17106887698173523,\n",
       "  0.17018043994903564,\n",
       "  0.16931714117527008,\n",
       "  0.1687057763338089,\n",
       "  0.16792990267276764,\n",
       "  0.16705560684204102,\n",
       "  0.16639703512191772,\n",
       "  0.16579724848270416,\n",
       "  0.16490115225315094,\n",
       "  0.1638917624950409,\n",
       "  0.16309618949890137,\n",
       "  0.1624964326620102,\n",
       "  0.16185398399829865,\n",
       "  0.1610933542251587,\n",
       "  0.15995632112026215,\n",
       "  0.1595718264579773,\n",
       "  0.15876248478889465,\n",
       "  0.15820938348770142,\n",
       "  0.15743522346019745,\n",
       "  0.15679962933063507,\n",
       "  0.15613266825675964,\n",
       "  0.15559855103492737,\n",
       "  0.15458035469055176,\n",
       "  0.1539021134376526,\n",
       "  0.15316686034202576,\n",
       "  0.15241806209087372,\n",
       "  0.15180230140686035,\n",
       "  0.15097345411777496,\n",
       "  0.15036055445671082,\n",
       "  0.14982081949710846,\n",
       "  0.1489195078611374,\n",
       "  0.14802545309066772,\n",
       "  0.14761078357696533,\n",
       "  0.14672145247459412,\n",
       "  0.14643198251724243,\n",
       "  0.14554892480373383,\n",
       "  0.14502425491809845,\n",
       "  0.14462213218212128,\n",
       "  0.14411994814872742,\n",
       "  0.1432412713766098,\n",
       "  0.14276844263076782,\n",
       "  0.14228416979312897,\n",
       "  0.14215973019599915,\n",
       "  0.14121480286121368,\n",
       "  0.14049701392650604,\n",
       "  0.1401737630367279,\n",
       "  0.13947446644306183,\n",
       "  0.13887201249599457,\n",
       "  0.13843949139118195,\n",
       "  0.13747256994247437,\n",
       "  0.13713093101978302,\n",
       "  0.13656888902187347,\n",
       "  0.13610845804214478,\n",
       "  0.13523852825164795,\n",
       "  0.1347847431898117,\n",
       "  0.13428933918476105,\n",
       "  0.13384540379047394,\n",
       "  0.13314363360404968,\n",
       "  0.13269968330860138,\n",
       "  0.13218137621879578,\n",
       "  0.13147380948066711,\n",
       "  0.13089467585086823,\n",
       "  0.1305716633796692,\n",
       "  0.1301109343767166,\n",
       "  0.12946449220180511,\n",
       "  0.12865257263183594,\n",
       "  0.12811331450939178,\n",
       "  0.12783537805080414,\n",
       "  0.12743811309337616,\n",
       "  0.12690390646457672,\n",
       "  0.12639060616493225,\n",
       "  0.12594573199748993,\n",
       "  0.12558613717556,\n",
       "  0.12487999349832535,\n",
       "  0.1244284063577652,\n",
       "  0.12391700595617294,\n",
       "  0.12361151725053787,\n",
       "  0.12309949100017548,\n",
       "  0.1223587766289711,\n",
       "  0.12182909995317459,\n",
       "  0.1212788075208664,\n",
       "  0.12092672288417816,\n",
       "  0.12061647325754166,\n",
       "  0.12030451744794846,\n",
       "  0.11969971656799316,\n",
       "  0.11912740021944046,\n",
       "  0.11880726367235184,\n",
       "  0.11836997419595718,\n",
       "  0.11794473230838776,\n",
       "  0.1176675483584404,\n",
       "  0.11719894409179688,\n",
       "  0.11628936231136322,\n",
       "  0.11539987474679947,\n",
       "  0.11517034471035004,\n",
       "  0.11472368985414505,\n",
       "  0.11400536447763443,\n",
       "  0.11373962461948395,\n",
       "  0.11343003809452057,\n",
       "  0.11265614628791809,\n",
       "  0.11223943531513214,\n",
       "  0.11167407035827637,\n",
       "  0.1109907254576683,\n",
       "  0.11032586544752121,\n",
       "  0.10959246754646301,\n",
       "  0.10912662744522095,\n",
       "  0.10872980207204819,\n",
       "  0.1079961359500885,\n",
       "  0.10737762600183487,\n",
       "  0.10667169839143753,\n",
       "  0.10612664371728897,\n",
       "  0.10572618246078491,\n",
       "  0.10483592748641968,\n",
       "  0.10442830622196198,\n",
       "  0.10372565686702728,\n",
       "  0.10308606177568436,\n",
       "  0.10255730152130127,\n",
       "  0.10186196863651276,\n",
       "  0.10114496201276779,\n",
       "  0.10080848634243011,\n",
       "  0.09990453720092773,\n",
       "  0.09944645315408707,\n",
       "  0.09868394583463669,\n",
       "  0.09824004024267197,\n",
       "  0.09768860787153244,\n",
       "  0.09706241637468338],\n",
       " 'accuracy': [0.8888888955116272,\n",
       "  0.8916666507720947,\n",
       "  0.8935185074806213,\n",
       "  0.8935185074806213,\n",
       "  0.894444465637207,\n",
       "  0.894444465637207,\n",
       "  0.894444465637207,\n",
       "  0.8962963223457336,\n",
       "  0.8962963223457336,\n",
       "  0.8972222208976746,\n",
       "  0.8972222208976746,\n",
       "  0.8990740776062012,\n",
       "  0.8990740776062012,\n",
       "  0.9009259343147278,\n",
       "  0.9037036895751953,\n",
       "  0.904629647731781,\n",
       "  0.9074074029922485,\n",
       "  0.9074074029922485,\n",
       "  0.9074074029922485,\n",
       "  0.9074074029922485,\n",
       "  0.9074074029922485,\n",
       "  0.9092592597007751,\n",
       "  0.9101851582527161,\n",
       "  0.9111111164093018,\n",
       "  0.9101851582527161,\n",
       "  0.9111111164093018,\n",
       "  0.9120370149612427,\n",
       "  0.9129629731178284,\n",
       "  0.9138888716697693,\n",
       "  0.914814829826355,\n",
       "  0.9138888716697693,\n",
       "  0.9166666865348816,\n",
       "  0.9157407283782959,\n",
       "  0.9166666865348816,\n",
       "  0.9157407283782959,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9157407283782959,\n",
       "  0.9157407283782959,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9157407283782959,\n",
       "  0.9185185432434082,\n",
       "  0.9185185432434082,\n",
       "  0.9194444417953491,\n",
       "  0.9203703999519348,\n",
       "  0.9212962985038757,\n",
       "  0.9222221970558167,\n",
       "  0.9222221970558167,\n",
       "  0.9222221970558167,\n",
       "  0.9231481552124023,\n",
       "  0.9231481552124023,\n",
       "  0.9231481552124023,\n",
       "  0.925000011920929,\n",
       "  0.925000011920929,\n",
       "  0.9259259104728699,\n",
       "  0.9240740537643433,\n",
       "  0.9259259104728699,\n",
       "  0.9277777671813965,\n",
       "  0.9268518686294556,\n",
       "  0.9287037253379822,\n",
       "  0.9296296238899231,\n",
       "  0.9296296238899231,\n",
       "  0.9296296238899231,\n",
       "  0.9305555820465088,\n",
       "  0.9305555820465088,\n",
       "  0.9314814805984497,\n",
       "  0.9314814805984497,\n",
       "  0.9314814805984497,\n",
       "  0.9314814805984497,\n",
       "  0.9314814805984497,\n",
       "  0.9324073791503906,\n",
       "  0.9333333373069763,\n",
       "  0.9351851940155029,\n",
       "  0.9333333373069763,\n",
       "  0.9351851940155029,\n",
       "  0.9342592358589172,\n",
       "  0.9351851940155029,\n",
       "  0.9361110925674438,\n",
       "  0.9370370507240295,\n",
       "  0.9370370507240295,\n",
       "  0.9370370507240295,\n",
       "  0.9370370507240295,\n",
       "  0.9361110925674438,\n",
       "  0.9370370507240295,\n",
       "  0.9379629492759705,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9416666626930237,\n",
       "  0.9416666626930237,\n",
       "  0.9435185194015503,\n",
       "  0.9416666626930237,\n",
       "  0.9444444179534912,\n",
       "  0.9444444179534912,\n",
       "  0.9444444179534912,\n",
       "  0.9444444179534912,\n",
       "  0.9453703761100769,\n",
       "  0.9462962746620178,\n",
       "  0.9462962746620178,\n",
       "  0.9462962746620178,\n",
       "  0.9472222328186035,\n",
       "  0.9481481313705444,\n",
       "  0.9490740895271301,\n",
       "  0.9481481313705444,\n",
       "  0.9490740895271301,\n",
       "  0.949999988079071,\n",
       "  0.9509259462356567,\n",
       "  0.9509259462356567,\n",
       "  0.9509259462356567,\n",
       "  0.9518518447875977,\n",
       "  0.9509259462356567,\n",
       "  0.9509259462356567,\n",
       "  0.9518518447875977,\n",
       "  0.9518518447875977,\n",
       "  0.9518518447875977,\n",
       "  0.9518518447875977,\n",
       "  0.9537037014961243,\n",
       "  0.9546296000480652,\n",
       "  0.9546296000480652,\n",
       "  0.9546296000480652,\n",
       "  0.9555555582046509,\n",
       "  0.9555555582046509,\n",
       "  0.9555555582046509,\n",
       "  0.9555555582046509,\n",
       "  0.9564814567565918,\n",
       "  0.9564814567565918,\n",
       "  0.9564814567565918,\n",
       "  0.9574074149131775,\n",
       "  0.9574074149131775,\n",
       "  0.9574074149131775,\n",
       "  0.9574074149131775,\n",
       "  0.9574074149131775,\n",
       "  0.9574074149131775,\n",
       "  0.9583333134651184,\n",
       "  0.9583333134651184,\n",
       "  0.9592592716217041,\n",
       "  0.9592592716217041,\n",
       "  0.9592592716217041,\n",
       "  0.960185170173645,\n",
       "  0.960185170173645,\n",
       "  0.960185170173645,\n",
       "  0.960185170173645,\n",
       "  0.960185170173645,\n",
       "  0.960185170173645,\n",
       "  0.960185170173645,\n",
       "  0.9611111283302307,\n",
       "  0.9611111283302307,\n",
       "  0.9620370268821716,\n",
       "  0.9629629850387573,\n",
       "  0.9620370268821716,\n",
       "  0.9611111283302307,\n",
       "  0.9620370268821716,\n",
       "  0.9611111283302307,\n",
       "  0.9611111283302307,\n",
       "  0.9611111283302307,\n",
       "  0.9611111283302307,\n",
       "  0.9620370268821716,\n",
       "  0.9620370268821716,\n",
       "  0.9638888835906982,\n",
       "  0.9620370268821716,\n",
       "  0.9629629850387573,\n",
       "  0.9629629850387573,\n",
       "  0.9629629850387573,\n",
       "  0.9629629850387573,\n",
       "  0.9629629850387573,\n",
       "  0.9629629850387573,\n",
       "  0.9629629850387573,\n",
       "  0.9629629850387573,\n",
       "  0.9629629850387573,\n",
       "  0.9638888835906982,\n",
       "  0.9648148417472839,\n",
       "  0.9657407402992249,\n",
       "  0.9648148417472839,\n",
       "  0.9657407402992249,\n",
       "  0.9648148417472839,\n",
       "  0.9648148417472839,\n",
       "  0.9657407402992249,\n",
       "  0.9648148417472839,\n",
       "  0.9648148417472839,\n",
       "  0.9657407402992249,\n",
       "  0.9638888835906982,\n",
       "  0.9648148417472839,\n",
       "  0.9638888835906982,\n",
       "  0.9648148417472839,\n",
       "  0.9657407402992249,\n",
       "  0.9666666388511658,\n",
       "  0.9657407402992249,\n",
       "  0.9657407402992249,\n",
       "  0.9657407402992249,\n",
       "  0.9657407402992249,\n",
       "  0.9657407402992249,\n",
       "  0.9666666388511658,\n",
       "  0.9666666388511658,\n",
       "  0.9675925970077515,\n",
       "  0.9675925970077515,\n",
       "  0.9675925970077515,\n",
       "  0.9675925970077515,\n",
       "  0.9675925970077515,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9675925970077515,\n",
       "  0.9675925970077515,\n",
       "  0.9675925970077515,\n",
       "  0.9675925970077515,\n",
       "  0.9675925970077515,\n",
       "  0.9666666388511658,\n",
       "  0.9675925970077515,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9694444537162781,\n",
       "  0.9685184955596924,\n",
       "  0.9694444537162781,\n",
       "  0.9694444537162781,\n",
       "  0.9694444537162781,\n",
       "  0.9694444537162781,\n",
       "  0.9694444537162781,\n",
       "  0.970370352268219,\n",
       "  0.9694444537162781,\n",
       "  0.9712963104248047,\n",
       "  0.9722222089767456,\n",
       "  0.9722222089767456,\n",
       "  0.9731481671333313,\n",
       "  0.9731481671333313,\n",
       "  0.9731481671333313,\n",
       "  0.9731481671333313,\n",
       "  0.9731481671333313,\n",
       "  0.9731481671333313,\n",
       "  0.9750000238418579,\n",
       "  0.9759259223937988,\n",
       "  0.9759259223937988,\n",
       "  0.9759259223937988,\n",
       "  0.9768518805503845,\n",
       "  0.9768518805503845,\n",
       "  0.9768518805503845,\n",
       "  0.9768518805503845,\n",
       "  0.9768518805503845,\n",
       "  0.9777777791023254],\n",
       " 'val_loss': [0.4372825026512146,\n",
       "  0.43522822856903076,\n",
       "  0.43402111530303955,\n",
       "  0.4329841732978821,\n",
       "  0.4314037263393402,\n",
       "  0.4296957552433014,\n",
       "  0.4285954535007477,\n",
       "  0.4276075065135956,\n",
       "  0.42554977536201477,\n",
       "  0.42459768056869507,\n",
       "  0.42373669147491455,\n",
       "  0.42218589782714844,\n",
       "  0.4208870232105255,\n",
       "  0.41984495520591736,\n",
       "  0.4190431535243988,\n",
       "  0.4170140326023102,\n",
       "  0.4164661467075348,\n",
       "  0.4150926470756531,\n",
       "  0.41369158029556274,\n",
       "  0.41333359479904175,\n",
       "  0.4127892851829529,\n",
       "  0.4114457964897156,\n",
       "  0.4106658697128296,\n",
       "  0.40975913405418396,\n",
       "  0.40829408168792725,\n",
       "  0.40785661339759827,\n",
       "  0.4069138169288635,\n",
       "  0.40630677342414856,\n",
       "  0.4053731858730316,\n",
       "  0.40436121821403503,\n",
       "  0.40441837906837463,\n",
       "  0.40296441316604614,\n",
       "  0.4030624330043793,\n",
       "  0.40163105726242065,\n",
       "  0.4012032449245453,\n",
       "  0.4010002911090851,\n",
       "  0.40016359090805054,\n",
       "  0.4000801742076874,\n",
       "  0.39950504899024963,\n",
       "  0.39795273542404175,\n",
       "  0.3984237611293793,\n",
       "  0.3975427746772766,\n",
       "  0.3972051739692688,\n",
       "  0.39707568287849426,\n",
       "  0.39591920375823975,\n",
       "  0.39648741483688354,\n",
       "  0.3958451449871063,\n",
       "  0.3953656554222107,\n",
       "  0.39476296305656433,\n",
       "  0.39423874020576477,\n",
       "  0.3942674994468689,\n",
       "  0.3940477967262268,\n",
       "  0.39370661973953247,\n",
       "  0.3938175141811371,\n",
       "  0.3929547071456909,\n",
       "  0.39244601130485535,\n",
       "  0.3917117714881897,\n",
       "  0.39181962609291077,\n",
       "  0.3915407061576843,\n",
       "  0.39204132556915283,\n",
       "  0.3917921781539917,\n",
       "  0.39138227701187134,\n",
       "  0.3909657895565033,\n",
       "  0.3907029330730438,\n",
       "  0.390510618686676,\n",
       "  0.3898595869541168,\n",
       "  0.38954958319664,\n",
       "  0.389134019613266,\n",
       "  0.3890247344970703,\n",
       "  0.38901644945144653,\n",
       "  0.3888823091983795,\n",
       "  0.3881172239780426,\n",
       "  0.3880784213542938,\n",
       "  0.3878369927406311,\n",
       "  0.3873363435268402,\n",
       "  0.3869844973087311,\n",
       "  0.3867059051990509,\n",
       "  0.3869224190711975,\n",
       "  0.3857296109199524,\n",
       "  0.3859337270259857,\n",
       "  0.38647177815437317,\n",
       "  0.3860412538051605,\n",
       "  0.38579508662223816,\n",
       "  0.3857763707637787,\n",
       "  0.38492074608802795,\n",
       "  0.3852173984050751,\n",
       "  0.3852686285972595,\n",
       "  0.384952574968338,\n",
       "  0.38473305106163025,\n",
       "  0.3836885690689087,\n",
       "  0.3848625719547272,\n",
       "  0.38436630368232727,\n",
       "  0.3843548595905304,\n",
       "  0.38347434997558594,\n",
       "  0.38487300276756287,\n",
       "  0.38454487919807434,\n",
       "  0.385262668132782,\n",
       "  0.3845931589603424,\n",
       "  0.385413259267807,\n",
       "  0.3847672641277313,\n",
       "  0.3857528269290924,\n",
       "  0.3845706880092621,\n",
       "  0.38568809628486633,\n",
       "  0.3852158784866333,\n",
       "  0.3860333561897278,\n",
       "  0.38612955808639526,\n",
       "  0.3860790431499481,\n",
       "  0.38691088557243347,\n",
       "  0.38745319843292236,\n",
       "  0.3871423900127411,\n",
       "  0.3870186507701874,\n",
       "  0.3873285949230194,\n",
       "  0.3873670697212219,\n",
       "  0.38715872168540955,\n",
       "  0.38785457611083984,\n",
       "  0.38800308108329773,\n",
       "  0.3874160051345825,\n",
       "  0.38797417283058167,\n",
       "  0.3884810507297516,\n",
       "  0.3872866630554199,\n",
       "  0.3883609175682068,\n",
       "  0.387423574924469,\n",
       "  0.38862210512161255,\n",
       "  0.38972827792167664,\n",
       "  0.38933536410331726,\n",
       "  0.3901529312133789,\n",
       "  0.3901713788509369,\n",
       "  0.3896016478538513,\n",
       "  0.39022353291511536,\n",
       "  0.39096972346305847,\n",
       "  0.39144909381866455,\n",
       "  0.3913559317588806,\n",
       "  0.3918350338935852,\n",
       "  0.39257940649986267,\n",
       "  0.39248162508010864,\n",
       "  0.3937157988548279,\n",
       "  0.3932671844959259,\n",
       "  0.39409148693084717,\n",
       "  0.39424431324005127,\n",
       "  0.3943929672241211,\n",
       "  0.3950107991695404,\n",
       "  0.39534857869148254,\n",
       "  0.3970898687839508,\n",
       "  0.39695024490356445,\n",
       "  0.39727333188056946,\n",
       "  0.3976454436779022,\n",
       "  0.39838477969169617,\n",
       "  0.3992909789085388,\n",
       "  0.3991336226463318,\n",
       "  0.3990655243396759,\n",
       "  0.4003738462924957,\n",
       "  0.40072524547576904,\n",
       "  0.4006017744541168,\n",
       "  0.4012562036514282,\n",
       "  0.40219536423683167,\n",
       "  0.40258923172950745,\n",
       "  0.40238699316978455,\n",
       "  0.4044376611709595,\n",
       "  0.4029562175273895,\n",
       "  0.4051778018474579,\n",
       "  0.4040476679801941,\n",
       "  0.40497052669525146,\n",
       "  0.4064048230648041,\n",
       "  0.4070424735546112,\n",
       "  0.40732434391975403,\n",
       "  0.4072073698043823,\n",
       "  0.40871620178222656,\n",
       "  0.4096779525279999,\n",
       "  0.4111132323741913,\n",
       "  0.4108196198940277,\n",
       "  0.4111446142196655,\n",
       "  0.4122644066810608,\n",
       "  0.4129345715045929,\n",
       "  0.4139585793018341,\n",
       "  0.41450148820877075,\n",
       "  0.41476961970329285,\n",
       "  0.4153648912906647,\n",
       "  0.4166337251663208,\n",
       "  0.4164610505104065,\n",
       "  0.4172477722167969,\n",
       "  0.41838300228118896,\n",
       "  0.41881635785102844,\n",
       "  0.4189826250076294,\n",
       "  0.4199645221233368,\n",
       "  0.42095819115638733,\n",
       "  0.4207340478897095,\n",
       "  0.42145127058029175,\n",
       "  0.42270511388778687,\n",
       "  0.423883855342865,\n",
       "  0.4243007004261017,\n",
       "  0.42465221881866455,\n",
       "  0.4244677722454071,\n",
       "  0.4260793626308441,\n",
       "  0.4264896512031555,\n",
       "  0.4273528754711151,\n",
       "  0.42790094017982483,\n",
       "  0.428149551153183,\n",
       "  0.4286482632160187,\n",
       "  0.42922842502593994,\n",
       "  0.4306979477405548,\n",
       "  0.42982998490333557,\n",
       "  0.4319823980331421,\n",
       "  0.43244239687919617,\n",
       "  0.4324365556240082,\n",
       "  0.43313243985176086,\n",
       "  0.43397605419158936,\n",
       "  0.4343535006046295,\n",
       "  0.43480584025382996,\n",
       "  0.43665599822998047,\n",
       "  0.4363774061203003,\n",
       "  0.4374832808971405,\n",
       "  0.43833234906196594,\n",
       "  0.43835973739624023,\n",
       "  0.43833014369010925,\n",
       "  0.43988996744155884,\n",
       "  0.44113093614578247,\n",
       "  0.44048812985420227,\n",
       "  0.4394540786743164,\n",
       "  0.43902862071990967,\n",
       "  0.4417712390422821,\n",
       "  0.4417385160923004,\n",
       "  0.4413340985774994,\n",
       "  0.443568617105484,\n",
       "  0.44089359045028687,\n",
       "  0.4423380494117737,\n",
       "  0.44249171018600464,\n",
       "  0.44191595911979675,\n",
       "  0.44187110662460327,\n",
       "  0.442436546087265,\n",
       "  0.44193699955940247,\n",
       "  0.44237595796585083,\n",
       "  0.4417179226875305,\n",
       "  0.44257113337516785,\n",
       "  0.4413924515247345,\n",
       "  0.44126737117767334,\n",
       "  0.4447997212409973,\n",
       "  0.44073373079299927,\n",
       "  0.4420815706253052,\n",
       "  0.44356659054756165,\n",
       "  0.4413670599460602,\n",
       "  0.44351235032081604,\n",
       "  0.4434562027454376,\n",
       "  0.4415108263492584,\n",
       "  0.4433971643447876,\n",
       "  0.44173288345336914,\n",
       "  0.4440945088863373,\n",
       "  0.441773384809494,\n",
       "  0.4434114098548889,\n",
       "  0.442883163690567,\n",
       "  0.44435080885887146],\n",
       " 'val_accuracy': [0.8166666626930237,\n",
       "  0.8166666626930237,\n",
       "  0.824999988079071,\n",
       "  0.824999988079071,\n",
       "  0.824999988079071,\n",
       "  0.824999988079071,\n",
       "  0.824999988079071,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8416666388511658,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8416666388511658,\n",
       "  0.8333333134651184,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8500000238418579,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8500000238418579,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8500000238418579,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8500000238418579,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8583333492279053,\n",
       "  0.8500000238418579,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.8666666746139526,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8583333492279053,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8583333492279053,\n",
       "  0.8500000238418579,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize the loss over time using `history.history`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'Accuracy'), Text(0.5, 0, 'Epoch')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The history.history[\"loss\"] entry is a dictionary with as many values as epochs that the\n",
    "# model was trained on. \n",
    "df_loss_acc = pd.DataFrame(history.history)\n",
    "df_loss= df_loss_acc[['loss','val_loss']]\n",
    "df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\n",
    "df_acc= df_loss_acc[['accuracy','val_accuracy']]\n",
    "df_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)\n",
    "df_loss.plot(title='Model loss',figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')\n",
    "df_acc.plot(title='Model Accuracy',figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations**! You've finished the assignment and built two models: One that recognizes  smiles, and another that recognizes SIGN language with almost 80% accuracy on the test set. In addition to that, you now also understand the applications of two Keras APIs: Sequential and Functional. Nicely done! \n",
    "\n",
    "By now, you know a bit about how the Functional API works and may have glimpsed the possibilities. In your next assignment, you'll really get a feel for its power when you get the opportunity to build a very deep ConvNet, using ResNets! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'happy_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhappy_model\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkraus.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'happy_model' is not defined"
     ]
    }
   ],
   "source": [
    "happy_model.save('kraus.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'download'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ca9814ca12a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhappy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kraus.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'download'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Bibliography\n",
    "\n",
    "You're always encouraged to read the official documentation. To that end, you can find the docs for the Sequential and Functional APIs here: \n",
    "\n",
    "https://www.tensorflow.org/guide/keras/sequential_model\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/functional"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "bwbJV",
   "launcher_item_id": "0TkXB"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
